{
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "celltoolbar": "Create Assignment",
    "colab": {
      "collapsed_sections": [],
      "name": "assignment3_gcp.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "a00e032c",
      "cell_type": "markdown",
      "source": "# The index building process",
      "metadata": {
        "id": "hWgiQS0zkWJ5"
      }
    },
    {
      "id": "5ac36d3a",
      "cell_type": "code",
      "source": "# if the following command generates an error, you probably didn't enable \n# the cluster security option \"Allow API access to all Google Cloud services\"\n# under Manage Security â†’ Project Access when setting up the cluster\n!gcloud dataproc clusters list --region us-central1",
      "metadata": {
        "id": "c0ccf76b",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-Worker_Count",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "cf88b954-f39a-412a-d87e-660833e735b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "NAME          PLATFORM  PRIMARY_WORKER_COUNT  SECONDARY_WORKER_COUNT  STATUS   ZONE           SCHEDULED_DELETE  SCHEDULED_STOP\r\n,cluster-0016  GCE       2                                             RUNNING  us-central1-a\r\n"
        }
      ],
      "execution_count": 2
    },
    {
      "id": "51cf86c5",
      "cell_type": "markdown",
      "source": "# Imports & Setup",
      "metadata": {
        "id": "01ec9fd3"
      }
    },
    {
      "id": "bf199e6a",
      "cell_type": "code",
      "source": "!pip install -q google-cloud-storage==1.43.0\n!pip install -q graphframes",
      "metadata": {
        "id": "32b3ec57",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-Setup",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "fc0e315d-21e9-411d-d69c-5b97e4e5d629"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n,\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n,\u001b[0m"
        }
      ],
      "execution_count": 3
    },
    {
      "id": "d8f56ecd",
      "cell_type": "code",
      "source": "import pyspark\nimport sys\nfrom collections import Counter, OrderedDict, defaultdict\nimport itertools\nfrom itertools import islice, count, groupby\nimport pandas as pd\nimport os\nimport re\nfrom operator import itemgetter\nimport nltk\nfrom nltk.stem.porter import *\nfrom nltk.corpus import stopwords\nfrom time import time\nfrom pathlib import Path\nimport pickle\nimport pandas as pd\nfrom google.cloud import storage\n\nimport hashlib\ndef _hash(s):\n    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n\nnltk.download('stopwords')",
      "metadata": {
        "id": "5609143b",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-Imports",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "a24aa24b-aa75-4823-83ca-1d7deef0f0de"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "[nltk_data] Downloading package stopwords to /root/nltk_data...\n,[nltk_data]   Package stopwords is already up-to-date!\n"
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "execution_count": 4
    },
    {
      "id": "38a897f2",
      "cell_type": "code",
      "source": "# if nothing prints here you forgot to include the initialization script when starting the cluster\n!ls -l /usr/lib/spark/jars/graph*",
      "metadata": {
        "id": "b10cc999",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-jar",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "8f93a7ec-71e0-49c1-fc81-9af385849a90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "-rw-r--r-- 1 root root 247882 Jan  8 15:04 /usr/lib/spark/jars/graphframes-0.8.2-spark3.1-s_2.12.jar\r\n"
        }
      ],
      "execution_count": 5
    },
    {
      "id": "47900073",
      "cell_type": "code",
      "source": "from pyspark.sql import *\nfrom pyspark.sql.functions import *\nfrom pyspark import SparkContext, SparkConf, SparkFiles\nfrom pyspark.sql import SQLContext\nfrom graphframes import *",
      "metadata": {
        "id": "d3f86f11",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-pyspark-import",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "execution_count": 6
    },
    {
      "id": "72bed56b",
      "cell_type": "code",
      "source": "spark",
      "metadata": {
        "id": "5be6dc2a",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-spark-version",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "07b4e22b-a252-42fb-fe46-d9050e4e7ca8",
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - hive</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://cluster-0016-m.c.roi-project-qa.internal:37703\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.3</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>yarn</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>PySparkShell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f25ef6c9e20>"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "execution_count": 7
    },
    {
      "id": "980e62a5",
      "cell_type": "code",
      "source": "# Put your bucket name below and make sure you can access it without an error\nbucket_name = 'roi-ir-bucket-1919' \nfull_path = f\"gs://{bucket_name}/\"\npaths=[]\n\nclient = storage.Client()\nblobs = client.list_blobs(bucket_name)\nfor b in blobs:\n    if b.name != 'graphframes.sh':\n        paths.append(full_path+b.name)",
      "metadata": {
        "id": "7adc1bf5",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-bucket_name",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "execution_count": 8
    },
    {
      "id": "cac891c2",
      "cell_type": "markdown",
      "source": "***GCP setup is complete!*** If you got here without any errors you've earned 10 out of the 35 points of this part.",
      "metadata": {
        "id": "13ZX4ervQkku"
      }
    },
    {
      "id": "582c3f5e",
      "cell_type": "markdown",
      "source": "# Building an inverted index",
      "metadata": {
        "id": "c0b0f215"
      }
    },
    {
      "id": "481f2044",
      "cell_type": "markdown",
      "source": "Here, we read the entire corpus to an rdd, directly from Google Storage Bucket and use your code from Colab to construct an inverted index.",
      "metadata": {
        "id": "02f81c72"
      }
    },
    {
      "id": "e4c523e7",
      "cell_type": "code",
      "source": "parquetFile = spark.read.parquet(*paths)\ndoc_text_pairs = parquetFile.select(\"text\", \"id\").rdd\ndoc2title = parquetFile.select(\"id\", \"title\").rdd",
      "metadata": {
        "id": "b1af29c9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "                                                                                \r"
        }
      ],
      "execution_count": 9
    },
    {
      "id": "d211f32e",
      "cell_type": "code",
      "source": "# saving RDD in the form of dict to retrive titles to their doc id\ndoc2title_dict = doc2title.collectAsMap()\nwith open('/./home/dataproc/doc2title_dict.pkl', 'wb') as file:\n    pickle.dump(doc2title_dict, file)\n\n!gsutil cp /home/dataproc/doc2title_dict.pkl gs://roi-ir-bucket-1919/",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "                                                                                \r"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Copying file:///home/dataproc/doc2title_dict.pkl [Content-Type=application/octet-stream]...\n,==> NOTE: You are uploading one or more large file(s), which would run          \n,significantly faster if you enable parallel composite uploads. This\n,feature can be enabled by editing the\n,\"parallel_composite_upload_threshold\" value in your .boto\n,configuration file. However, note that if you do this large files will\n,be uploaded as `composite objects\n,<https://cloud.google.com/storage/docs/composite-objects>`_,which\n,means that any user who downloads such objects will need to have a\n,compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n,without a compiled crcmod, computing checksums on composite objects is\n,so slow that gsutil disables downloads of composite objects.\n,\n,| [1 files][168.9 MiB/168.9 MiB]                                                \n,Operation completed over 1 objects/168.9 MiB.                                    \n"
        }
      ],
      "execution_count": 10
    },
    {
      "id": "e535e4ad",
      "cell_type": "code",
      "source": "print(list(doc2title_dict.items())[:2])",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "[(4045403, 'Foster Air Force Base'), (4045413, 'Torino Palavela')]\n"
        }
      ],
      "execution_count": 11
    },
    {
      "id": "0d7e2971",
      "cell_type": "markdown",
      "source": "We will count the number of pages to make sure we are looking at the entire corpus. The number of pages should be more than 6M",
      "metadata": {
        "id": "f6375562"
      }
    },
    {
      "id": "82881fbf",
      "cell_type": "code",
      "source": "# Count number of wiki pages\nparquetFile.count()",
      "metadata": {
        "id": "d89a7a9a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "                                                                                \r"
        },
        {
          "data": {
            "text/plain": [
              "6348910"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "execution_count": 12
    },
    {
      "id": "701811af",
      "cell_type": "markdown",
      "source": "Let's import the inverted index module. Note that you need to use the staff-provided version called `inverted_index_gcp.py`, which contains helper functions to writing and reading the posting files similar to the Colab version, but with writing done to a Google Cloud Storage bucket.",
      "metadata": {
        "id": "gaaIoFViXyTg"
      }
    },
    {
      "id": "121fe102",
      "cell_type": "code",
      "source": "# if nothing prints here you forgot to upload the file inverted_index_gcp.py to the home dir\n%cd -q /home/dataproc\n!ls inverted_index_gcp.py",
      "metadata": {
        "id": "04371c88",
        "outputId": "327fe81b-80f4-4b3a-8894-e74720d92e35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "inverted_index_gcp.py\r\n"
        }
      ],
      "execution_count": 13
    },
    {
      "id": "57c101a8",
      "cell_type": "code",
      "source": "# adding our python module to the cluster\nsc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\nsys.path.insert(0,SparkFiles.getRootDirectory())",
      "metadata": {
        "id": "2d3285d8",
        "scrolled": true
      },
      "outputs": [],
      "execution_count": 14
    },
    {
      "id": "c259c402",
      "cell_type": "code",
      "source": "from inverted_index_gcp import InvertedIndex",
      "metadata": {
        "id": "2477a5b9"
      },
      "outputs": [],
      "execution_count": 15
    },
    {
      "id": "5540c727",
      "cell_type": "markdown",
      "source": "**Tokenization & More**",
      "metadata": {
        "id": "72bcf46a"
      }
    },
    {
      "id": "f3ad8fea",
      "cell_type": "code",
      "source": "english_stopwords = frozenset(stopwords.words('english'))\ncorpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\", \n                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\", \n                    \"part\", \"thumb\", \"including\", \"second\", \"following\", \n                    \"many\", \"however\", \"would\", \"became\"]\n\nall_stopwords = english_stopwords.union(corpus_stopwords)\nRE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n\nNUM_BUCKETS = 124\ndef token2bucket_id(token):\n  return int(_hash(token),16) % NUM_BUCKETS\n\ndef word_count(text, id):\n  ''' Count the frequency of each word in `text` (tf) that is not included in\n  `all_stopwords` and return entries that will go into our posting lists.\n  Parameters:\n  -----------\n    text: str\n      Text of one document\n    id: int\n      Document id\n  Returns:\n  --------\n    List of tuples\n      A list of (token, (doc_id, tf)) pairs\n      for example: [(\"Anarchism\", (12, 5)), ...]\n  '''\n  tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n  filtered_tokens = []\n  word_count_list = []\n  for token in tokens:\n    if token not in all_stopwords: #first we take out all the stop words\n      filtered_tokens.append(token)\n  word_count = {} #we create a dict for the word counts\n  for token in filtered_tokens:\n    word_count[token] = word_count.get(token, 0) + 1 #we count each word \n  for token, count in word_count.items(): #we loop over the dict\n    word_count_list.append((token, (id, count))) #then append to list\n  return word_count_list #according to the asked pattern\n\ndef reduce_word_counts(unsorted_pl):\n  ''' Returns a sorted posting list by wiki_id.\n  Parameters:\n  -----------\n    unsorted_pl: list of tuples\n      A list of (wiki_id, tf) tuples\n  Returns:\n  --------\n    list of tuples\n      A sorted posting list.\n  '''\n  sorted_posting_list = sorted(unsorted_pl, key=lambda x: x[0])\n  return sorted_posting_list\n\ndef calculate_df(postings):\n  ''' Takes a posting list RDD and calculate the df for each token.\n  Parameters:\n  -----------\n    postings: RDD\n      An RDD where each element is a (token, posting_list) pair.\n  Returns:\n  --------\n    RDD\n      An RDD where each element is a (token, df) pair.\n  '''\n  return postings.mapValues(lambda posting_list: len(posting_list)) #we count the number of times each word apear in a document\n\n\ndef partition_postings_and_write(postings):\n  ''' A function that partitions the posting lists into buckets, writes out\n  all posting lists in a bucket to disk, and returns the posting locations for\n  each bucket. Partitioning should be done through the use of `token2bucket`\n  above. Writing to disk should use the function  `write_a_posting_list`, a\n  static method implemented in inverted_index_colab.py under the InvertedIndex\n  class.\n  Parameters:\n  -----------\n    postings: RDD\n      An RDD where each item is a (w, posting_list) pair.\n  Returns:\n  --------\n    RDD\n      An RDD where each item is a posting locations dictionary for a bucket. The\n      posting locations maintain a list for each word of file locations and\n      offsets its posting list was written to. See `write_a_posting_list` for\n      more details.\n  '''\n  # first we map each token to the bucket (bucket id, (token,posting_list))\n  token_to_bucket = postings.map(lambda x: (token2bucket_id(x[0]),(x[0],x[1])))\n  #we group each token by bucket id \n  group_by_bucketID = token_to_bucket.groupByKey()\n  # we write each bucket to the disk\n  # we pack (bucket_id, list_of_terms) into one tuple \n  # and then write_a_posting_list wirtes the bucket to file and returns\n  # a dictionary mapping each token to the locations of its posting list\n  bucket_to_disk = group_by_bucketID.map(lambda bucket: \n                                         InvertedIndex.write_a_posting_list(\n                                             (bucket[0],list(bucket[1])),bucket_name))\n  return bucket_to_disk",
      "metadata": {
        "id": "a4b6ee29",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-token2bucket",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "execution_count": 16
    },
    {
      "id": "55c8764e",
      "cell_type": "code",
      "source": "# time the index creation time\nt_start = time()\n# word counts map\nword_counts = doc_text_pairs.flatMap(lambda x: word_count(x[0], x[1]))\npostings = word_counts.groupByKey().mapValues(reduce_word_counts)\n# filtering postings and calculate df\npostings_filtered = postings.filter(lambda x: len(x[1])>50)\nw2df = calculate_df(postings_filtered)\nw2df_dict = w2df.collectAsMap()\ndoc_len_rdd = doc_text_pairs.map(\n    lambda x: (x[0], len(str(x[1]).split()))\n)\ndoc_lengths = doc_text_pairs.map(lambda x: len(str(x[1]).split()))\navg_doc_length = doc_lengths.mean()\nwith open(\"avg_doc_len.pkl\", \"wb\") as f:\n    pickle.dump(avg_doc_length, f)\n\n!gsutil cp avg_doc_len.pkl gs://roi-ir-bucket-1919/avg_doc_len.pkl\n\n# partition posting lists and write out\n_ = partition_postings_and_write(postings_filtered).collect()\nindex_const_time = time() - t_start",
      "metadata": {
        "id": "0b5d7296",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-index_construction",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "                                                                                \r"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Copying file://avg_doc_len.pkl [Content-Type=application/octet-stream]...\n,/ [1 files][   21.0 B/   21.0 B]                                                \n,Operation completed over 1 objects/21.0 B.                                       \n"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "                                                                                \r"
        }
      ],
      "execution_count": null
    },
    {
      "id": "3dbc0e14",
      "cell_type": "code",
      "source": "# test index construction time\nassert index_const_time < 60*120",
      "metadata": {
        "id": "348pECY8cH-T",
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-index_const_time",
          "locked": true,
          "points": 10,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "execution_count": 18
    },
    {
      "id": "ab3296f4",
      "cell_type": "code",
      "source": "# collect all posting lists locations into one super-set\nsuper_posting_locs = defaultdict(list)\nfor blob in client.list_blobs(bucket_name, prefix='postings_gcp'):\n  if not blob.name.endswith(\"pickle\"):\n    continue\n  with blob.open(\"rb\") as f:\n    posting_locs = pickle.load(f)\n    for k, v in posting_locs.items():\n      super_posting_locs[k].extend(v)",
      "metadata": {
        "id": "Opl6eRNLM5Xv",
        "nbgrader": {
          "grade": true,
          "grade_id": "collect-posting",
          "locked": true,
          "points": 0,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "execution_count": 19
    },
    {
      "id": "f6f66e3a",
      "cell_type": "markdown",
      "source": "Putting it all together",
      "metadata": {
        "id": "VhAV0A6dNZWY"
      }
    },
    {
      "id": "a5d2cfb6",
      "cell_type": "code",
      "source": "# Create inverted index instance\ninverted = InvertedIndex()\n# Adding the posting locations dictionary to the inverted index\ninverted.posting_locs = super_posting_locs\n# Add the token - df dictionary to the inverted index\ninverted.df = w2df_dict\n# write the global stats out\ninverted.write_index('.', 'index')\n# upload to gs\nindex_src = \"index.pkl\"\nindex_dst = f'gs://{bucket_name}/postings_gcp/{index_src}'\n!gsutil cp $index_src $index_dst",
      "metadata": {
        "id": "54vqT_0WNc3w"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Copying file://index.pkl [Content-Type=application/octet-stream]...\n,/ [1 files][ 18.4 MiB/ 18.4 MiB]                                                \n,Operation completed over 1 objects/18.4 MiB.                                     \n"
        }
      ],
      "execution_count": 20
    },
    {
      "id": "8f880d59",
      "cell_type": "code",
      "source": "!gsutil ls -lh $index_dst",
      "metadata": {
        "id": "msogGbJ3c8JF",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-index_dst_size",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": " 18.45 MiB  2026-01-09T20:32:41Z  gs://roi-ir-bucket-1919/postings_gcp/index.pkl\r\n,TOTAL: 1 objects, 19343760 bytes (18.45 MiB)\r\n"
        }
      ],
      "execution_count": 21
    },
    {
      "id": "c52dee14",
      "cell_type": "markdown",
      "source": "# PageRank",
      "metadata": {
        "id": "fc0667a9",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-2a6d655c112e79c5",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      }
    },
    {
      "id": "0875c6bd",
      "cell_type": "markdown",
      "source": "**YOUR TASK (10 POINTS):** Compute PageRank for the entire English Wikipedia. Use your implementation for `generate_graph` function from Colab below.",
      "metadata": {
        "id": "fdd1bdca",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-2fee4bc8d83c1e2a",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      }
    },
    {
      "id": "31a516e2",
      "cell_type": "code",
      "source": "def generate_graph(pages):\n  ''' Compute the directed graph generated by wiki links.\n  Parameters:\n  -----------\n    pages: RDD\n      An RDD where each row consists of one wikipedia articles with 'id' and\n      'anchor_text'.\n  Returns:\n  --------\n    edges: RDD\n      An RDD where each row represents an edge in the directed graph created by\n      the wikipedia links. The first entry should the source page id and the\n      second entry is the destination page id. No duplicates should be present.\n    vertices: RDD\n      An RDD where each row represents a vetrix (node) in the directed graph\n      created by the wikipedia links. No duplicates should be present.\n  '''\n  # we create the edeges and for each page (row), we produce pairs where each\n  # is holding source_id and destination_id\n  edges = pages.flatMap(lambda row: [(row.id, link.id) for link in row.anchor_text])\n  # we remove duplicates\n  edges = edges.distinct()\n  # we create a set of vertices that appear as source pages\n  source_vertices = pages.map(lambda row:(row.id,))\n  # we create a set of vertices that appear as destination pages\n  # can have multiple links\n  destination_vertices = pages.flatMap(lambda row: [(link.id,) for link in row.anchor_text])\n  # we combine them and remove duplicates\n  vertices = source_vertices.union(destination_vertices).distinct()\n  return edges, vertices\n",
      "metadata": {
        "id": "yVjnTvQsegc-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "6bc05ba3",
      "cell_type": "code",
      "source": "t_start = time()\npages_links = spark.read.parquet(\"gs://roi-ir-bucket-1919/multistream*\").select(\"id\", \"anchor_text\").rdd\n# construct the graph \nedges, vertices = generate_graph(pages_links)\n# compute PageRank\nedgesDF = edges.toDF(['src', 'dst']).repartition(124, 'src')\nverticesDF = vertices.toDF(['id']).repartition(124, 'id')\ng = GraphFrame(verticesDF, edgesDF)\npr_results = g.pageRank(resetProbability=0.15, maxIter=6)\npr_rdd = pr_results.vertices \\\n    .select(\"id\", \"pagerank\") \\\n    .rdd \\\n    .map(lambda r: (int(r.id), float(r.pagerank)))\n\n# Convert to dictionary\npagerank_dict = pr_rdd.collectAsMap()\n\n# Save locally as a pickle file\nwith open(\"pagerank.pkl\", \"wb\") as f:\n    pickle.dump(pagerank_dict, f)\n\n# Upload to GCP bucket using gsutil\ndest_path = f\"gs://{bucket_name}/pagerank.pkl\"\n!gsutil cp pagerank.pkl $dest_path\n\npr_time = time() - t_start\npr_results.vertices.select(\"id\", \"pagerank\").show(5)\n\nprint(f\"PageRank Finished! Time: {pr_time:.2f}s\")\nprint(f\"Total pages ranked: {len(pagerank_dict)}\")",
      "metadata": {
        "id": "db005700",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-PageRank",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "bacef13a-4190-4ff7-a652-2fd1b9c65368",
      "cell_type": "code",
      "source": "t_start = time()\npages_links = spark.read.parquet(\"gs://roi-ir-bucket-1919/multistream*\").select(\"id\", \"anchor_text\").rdd\n# construct the graph \nedges, vertices = generate_graph(pages_links)\n# compute PageRank\nedgesDF = edges.toDF(['src', 'dst']).repartition(124, 'src')\nverticesDF = vertices.toDF(['id']).repartition(124, 'id')\ng = GraphFrame(verticesDF, edgesDF)\npr_results = g.pageRank(resetProbability=0.15, maxIter=6)\npr_rdd = pr_results.vertices \\\n    .select(\"id\", \"pagerank\") \\\n    .rdd \\\n    .map(lambda r: (int(r.id), float(r.pagerank)))\n\n# Convert to dictionary\npagerank_dict = pr_rdd.collectAsMap()\n\n# Save locally as a pickle file\nwith open(\"pagerank.pkl\", \"wb\") as f:\n    pickle.dump(pagerank_dict, f)\n\n# Upload to GCP bucket using gsutil\ndest_path = f\"gs://{bucket_name}/pagerank.pkl\"\n!gsutil cp pagerank.pkl $dest_path\n\npr_time = time() - t_start\npr_results.vertices.select(\"id\", \"pagerank\").show(5)\n\nprint(f\"PageRank Finished! Time: {pr_time:.2f}s\")\nprint(f\"Total pages ranked: {len(pagerank_dict)}\")",
      "metadata": {
        "id": "db005700",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-PageRank",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "                                                                                \r"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Copying file://pagerank.pkl [Content-Type=application/octet-stream]...\n,| [1 files][ 84.7 MiB/ 84.7 MiB]                                                \n,Operation completed over 1 objects/84.7 MiB.                                     \n"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "[Stage 227:=============================================>       (107 + 8) / 124]\r"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "+----+------------------+\n,|  id|          pagerank|\n,+----+------------------+\n,|1677| 40.60299390449796|\n,|1697| 6.725054306196098|\n,|1806| 71.23858462582163|\n,|1950| 94.42358111798255|\n,|2250|2.5674204862854544|\n,+----+------------------+\n,only showing top 5 rows\n,\n,PageRank Finished! Time: 6328.52s\n,Total pages ranked: 6348983\n"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "                                                                                \r"
        }
      ],
      "execution_count": null
    },
    {
      "id": "f7717604",
      "cell_type": "code",
      "source": "# test that PageRank computaion took less than 1 hour\nassert pr_time < 60*120",
      "metadata": {
        "id": "2cc36ca9",
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-PageRank_time",
          "locked": true,
          "points": 10,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}